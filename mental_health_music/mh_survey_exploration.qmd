---
title: "Mental Health and Musical Preferences"
author: "Meghan Harris"
format: html
engine: knitr
execute:
  echo: true
  warning: false
  error: true
code-fold: true
# Extension configuration
filters:
  - collapse-output
extensions:
  collapse-output:
    method: javascript
format-options:
  python:
    code-formatter: none
---

# Introduction

I haven't done statistical modeling since 2021, and back then I tried using base R's `lm()` and hated it. I didn't know tidymodels existed at the time, so I assumed modeling just wasn't for me. Fast forward to 2024: I'm pivoting from data science to open source development, learning Python alongside R, and decided to revisit modeling with modern tools. This notebook compares a simple ML workflow in both R (tidymodels) and Python (scikit-learn) to:

1. Refresh my modeling skills using tidymodels (which actually makes sense to my brain)
2. Practice Python by replicating a workflow I understand in R

I'm using the [Music & Mental Health Survey Results](https://www.kaggle.com/datasets/catherinerasgaitis/mxmh-survey-results) dataset from Kaggle to predict anxiety scores from music listening habits.


# Data Import and Viewing
First, we import the data.

::: {.panel-tabset}

## R

```{r, r-data-import}
# install.packages(c("janitor", "tidyverse", "tidymodels"))
library(tidymodels)
library(tidyverse)

# Grab relative data path
data_path <-
  here::here(
    "mental_health_music",
    "data",
    "mxmh_survey_results.csv"
  )

# Import data and clean vars
df_mh_music_r <-
  read_csv(data_path) |>
  janitor::clean_names()

```

## Python

```{python, py-data-import}
import polars as pl
import janitor.polars
from pathlib import Path

# Python has been SUPER annoying with paths for me in quarto docs
# Maybe it's me/my machine but, meh
# Setting two possible locations for the data file
possible_paths = [
    Path("data") / "mxmh_survey_results.csv",
    Path("mental_health_music") / "data" / "mxmh_survey_results.csv",
]

for path in possible_paths:
    if path.exists():
        data_path = path
        break
else:
    raise FileNotFoundError("mxmh_survey_results.csv not found in expected locations.")

# Read in csv
df_mh_music_py = pl.read_csv(
    data_path,
    # This is needed because there's decimals in the
    # scoring cols where integer was inferred
    infer_schema_length=1000,
)

# Clean the names
df_mh_music_py = df_mh_music_py.clean_names(remove_special=True)
```

:::

Then, do a little skimming with glimpse...

::: {.panel-tabset collapse=true}

## R

```{r, r-data-glimpse}
#| output-fold: true
library(dplyr)

# Take a peek
df_mh_music_r |>
  glimpse()

```

## Python

```{python, py-data-glimpse}
# | output-fold: true
# Take a peek
df_mh_music_py.glimpse()
```

:::

# Data Exploration

Looking at the data, I'm interested in predicting "anxiety score" from musical listening patterns with respect to genre frequencies.
We'll need the `anxiety`, `frequency_*` and maybe the `hours_per_day` variables, so let's visualize what we have here.

For the anxiety score interpretation on a scale from 0 to 10, respondents where asked "how often do you experience this?":
  - 0 - I do not experience this.
  - 10 - I experience this regularly, constantly/or to an extreme.

::: {.panel-tabset collapse=true}

## R

So, of no surprise to me, this data is right-skewed with a concentration sitting around a score from 7 - 8.
The density curve also shows limited variability at lower anxiety levels ("flattening" of the curve), which may affect model performance in predicting across the full range of the outcome.

```{r, r-eda}
df_mh_music_r |>
  ggplot(aes(x = anxiety)) +
  geom_histogram(
    aes(y = after_stat(density), fill = after_stat(count)),
    bins = 11,
    color = "#000000"
  ) +
  geom_density(color = "#000000", size = 3) +
  geom_density(color = "#a873d1", size = 1) +
  scale_fill_gradient2(
    low = "#ffffff",
    high = "#250647"
  ) +
  labs(
    title = "Distribution of Self-Reported Anxiety Scores",
    subtitle = "Among Survey Respondents",
    fill = "Total Responses:",
    x = "Reported Anxiety Score",
    y = "Density"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    text = element_text(size = 10),
    legend.position = "top"
  )
```

## Python

I'm genuinely curious to see how ages play out with this. Which generation has the most anxiety on average? This data was first uploaded onto Kaggle three years ago; So I'll use the year of 2022 to create categories of generations like "Millennials", "Gen-Z", etc. We'll calculate it based on this random table Google generated:

| Generation Name             | Birth Years    | Current Age Range (as of 2022)      |
|-----------------------------|---------------|--------------------------------------|
| The Silent Generation       | 1928–1945     | 77–94 years old                      |
| Baby Boomers                | 1946–1964     | 58–76 years old                      |
| Generation X (Gen X)        | 1965–1980     | 42–57 years old                      |
| Millennials (Gen Y)         | 1981–1996     | 26–41 years old                      |
| Generation Z (Gen Z)        | 1997–2012     | 10–25 years old                      |
| Generation Alpha (Gen Alpha)| 2013–2024     | 0–9 years old                        |

```{python}
# I feel like there has to be a better way as oppose to listing a whole bunch of if/then statements
# I'm ditching readability right now to get some XP using .cut
# lol cut was labelled as unstable --- yolo
generation_buckets = [1927, 1945, 1964, 1980, 1996, 2012, 2024]  # 7 edges
generation_labels = [
    "The Silent Generation",
    "Baby Boomers",
    "Gen-X",
    "Millennials",
    "Gen-Z",
    "Gen-Alpha",
]  # 6 labels

# Drop rows where age is missing
df_mh_music_py = df_mh_music_py.filter(pl.col("age").is_not_null())

# CHeck the ranges of ages//derived birth_year

```

:::
# Data Cleaning 

::: {.panel-tabset collapse=true}

## R

```{r}

```

## Python

```{python}


```

:::

# Train/Test Split

::: {.panel-tabset collapse=true}

## R

```{r}

```

## Python

```{python}


```

:::


# Model Building

## Recipe Definition

::: {.panel-tabset collapse=true}

## R

```{r}

```

## Python

```{python}


```

:::

## Model Specification

::: {.panel-tabset collapse=true}

## R

```{r}

```

## Python

```{python}


```

:::

## Workflow Creation

::: {.panel-tabset collapse=true}

## R

```{r}

```

## Python

```{python}


```

:::

# Model Tuning

## Cross-Validation Setup
::: {.panel-tabset collapse=true}

## R

```{r}

```

## Python

```{python}


```

:::

## Tuning Grid & Selection
::: {.panel-tabset collapse=true}

## R

```{r}

```

## Python

```{python}


```

:::

# Final Model Tuning

::: {.panel-tabset collapse=true}

## R

```{r}

```

## Python

```{python}


```

:::

# Model Evaluation

::: {.panel-tabset collapse=true}

## R

```{r}

```

## Python

```{python}


```

:::

# Comparison & Takeaways

# Conclusion

```{python, scratch-code}
#| echo: false
#| eval: false

import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
import plotnine as p9
```